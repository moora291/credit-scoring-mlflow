{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac459a09",
   "metadata": {},
   "source": [
    "# Step 1 - Data prep and cleaning\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e3a4c",
   "metadata": {},
   "source": [
    "## 1. Load data files\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf0c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "raw_data_path = Path('../data/raw/')\n",
    "files = list(raw_data_path.glob('*.csv'))\n",
    "datasets = {}\n",
    "\n",
    "# Encodage utf-8 avec gestion des erreurs\n",
    "for file in files:\n",
    "    try:\n",
    "        df = pd.read_csv(file, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(file, encoding='ISO-8859-1')\n",
    "    datasets[file.stem] = df\n",
    "    print(f\"{file.name}: {df.shape[0]} lignes, {df.shape[1]} colonnes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2da86",
   "metadata": {},
   "source": [
    "## 2. Validate and fix formats\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3485653",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in datasets.items():\n",
    "    print(f\"\\n==== {name.upper()} : types initiaux ====\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    # Exemple : conversion des colonnes 'DAYS_BIRTH' en valeur absolue\n",
    "    if 'DAYS_BIRTH' in df.columns:\n",
    "        df['DAYS_BIRTH'] = df['DAYS_BIRTH'].abs()\n",
    "    \n",
    "    # Conversion des colonnes contenant 'DATE' ou 'datetime' si besoin\n",
    "    for col in df.select_dtypes('object'):\n",
    "        if 'date' in col.lower() or 'datetime' in col.lower():\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col])\n",
    "                print(f\"{col} convertie en datetime\")\n",
    "            except:\n",
    "                pass  # ignore if not convertible\n",
    "\n",
    "    # Update the dictionary\n",
    "    datasets[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a9ce96",
   "metadata": {},
   "source": [
    "## 3. Missing values and duplicates\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c075bbd",
   "metadata": {},
   "source": [
    "### 3.1 Visualize missing values\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22afded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    na_ratio = df.isna().mean().sort_values(ascending=False)\n",
    "    print(\"Variables with > 50% missing values:\")\n",
    "    display(na_ratio[na_ratio > 0.5])\n",
    "\n",
    "    msno.matrix(df)\n",
    "    plt.title(f\"Missing values - {name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8541cc0",
   "metadata": {},
   "source": [
    "### 3.2 Cleaning: drop and impute\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e62482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n=== Cleaning dataset : {name.upper()} ===\")\n",
    "\n",
    "    # 1. Supprimer les colonnes avec plus de 80% de NaN\n",
    "    threshold = 0.8\n",
    "    missing_ratio = df.isna().mean()\n",
    "    cols_to_drop = missing_ratio[missing_ratio > threshold].index\n",
    "    df.drop(columns=cols_to_drop, inplace=True)\n",
    "    print(f\"{len(cols_to_drop)} columns dropped (>80% NaN) : {list(cols_to_drop)}\")\n",
    "\n",
    "    # 2. Impute numeric variables (median)\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    imputer_num = SimpleImputer(strategy='median')\n",
    "    df[num_cols] = imputer_num.fit_transform(df[num_cols])\n",
    "\n",
    "    # 3. Impute categorical variables (use \"unknown\")\n",
    "    cat_cols = df.select_dtypes(include='object').columns\n",
    "    df[cat_cols] = df[cat_cols].fillna(\"inconnu\")\n",
    "\n",
    "    # Update the dictionary\n",
    "    datasets[name] = df\n",
    "\n",
    "    print(f\"{len(num_cols)} numeric columns imputed\")\n",
    "    print(f\"{len(cat_cols)} categorical columns imputed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2a8a00",
   "metadata": {},
   "source": [
    "### 3.3 Remove duplicates\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e962ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in datasets.items():\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        print(f\"{name}: {duplicates} duplicates removed\")\n",
    "    else:\n",
    "        print(f\"{name}: no duplicates detected\")\n",
    "\n",
    "    # Update\n",
    "    datasets[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0214fc90",
   "metadata": {},
   "source": [
    "## 4. Identify join keys\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57768dc0",
   "metadata": {},
   "source": [
    "## Join keys\n\n",
    "Details and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d38fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join keys identified manually from the documentation\n",
    "primary_keys = {\n",
    "    'application_train': 'SK_ID_CURR',\n",
    "    'application_test': 'SK_ID_CURR',\n",
    "    'bureau': 'SK_ID_CURR',\n",
    "    'bureau_balance': 'SK_ID_BUREAU',\n",
    "    'previous_application': 'SK_ID_CURR',\n",
    "    'installments_payments': 'SK_ID_PREV',\n",
    "    'credit_card_balance': 'SK_ID_PREV',\n",
    "    'POS_CASH_balance': 'SK_ID_PREV',\n",
    "    'sample_submission': 'SK_ID_CURR'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1544efaa",
   "metadata": {},
   "source": [
    "## 5. Merge datasets\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90bb719",
   "metadata": {},
   "source": [
    "### 5.1 Merge bureau data\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d53ae88-bff4-4b36-b9f6-7a3a9c5b5d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_numeric_only(df, group_var, df_name):\n",
    "    \"\"\"\n",
    "    Aggregate numeric columns of a DataFrame by a given key.\n",
    "    Column names are prefixed to avoid collisions.\n",
    "    \"\"\"\n",
    "    df_numeric = df.select_dtypes(include=['number']).copy()\n",
    "    df_numeric[group_var] = df[group_var]\n",
    "    agg = df_numeric.groupby(group_var).agg(['mean', 'sum'])\n",
    "    agg.columns = [f\"{df_name}_{col[0]}_{col[1]}\" for col in agg.columns]\n",
    "    agg.reset_index(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5489bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge bureau_balance into bureau\n",
    "bureau_balance = datasets['bureau_balance']\n",
    "bureau = datasets['bureau']\n",
    "\n",
    "bureau_balance_agg = agg_numeric_only(bureau_balance, 'SK_ID_BUREAU', 'BB')\n",
    "bureau = bureau.merge(bureau_balance_agg, on='SK_ID_BUREAU', how='left')\n",
    "\n",
    "# Merge bureau into application\n",
    "app_train = datasets['application_train']\n",
    "app_train = app_train.merge(bureau.drop(columns='SK_ID_BUREAU'), on='SK_ID_CURR', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13149e",
   "metadata": {},
   "source": [
    "### 5.2 Merge previous application histories\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8eed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous = datasets['previous_application']\n",
    "\n",
    "# Aggregates for histories linked to previous applications\n",
    "for table_name in ['POS_CASH_balance', 'installments_payments', 'credit_card_balance']:\n",
    "    df = datasets[table_name]\n",
    "    df_agg = agg_numeric_only(df, 'SK_ID_PREV', table_name)\n",
    "    previous = previous.merge(df_agg, on='SK_ID_PREV', how='left')\n",
    "\n",
    "# Merge with application\n",
    "app_train = app_train.merge(previous.drop(columns='SK_ID_PREV'), on='SK_ID_CURR', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8298c1",
   "metadata": {},
   "source": [
    "### 5.3 Final export\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path('../data/output')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "app_train.to_csv(output_path / 'train_clean_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac5da0e",
   "metadata": {},
   "source": [
    "## Sampling note\n\n",
    "The full merged dataset is very large. For local experiments, a smaller sample is used to keep notebook execution practical.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b98069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Count total rows (minus header)\n",
    "with open(\"../data/output/train_clean_merged.csv\", \"r\") as f:\n",
    "    total_lines = sum(1 for line in f) - 1\n",
    "\n",
    "# Calculer combien garder (1/8 du fichier)\n",
    "eighth_lines = total_lines // 8\n",
    "\n",
    "# Lire et sauvegarder 1/8 des data\n",
    "df_sample = pd.read_csv(\"../data/output/train_clean_merged.csv\", nrows=eighth_lines)\n",
    "df_sample.to_csv(\"../data/output/train_clean_sample.csv\", index=False)\n",
    "\n",
    "# Supprimer le fichier trop volumineux\n",
    "import os\n",
    "os.remove(\"../data/output/train_clean_merged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92a0110",
   "metadata": {},
   "source": [
    "## Conclusion\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "credit-scoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}