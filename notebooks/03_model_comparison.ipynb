{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e209691a",
   "metadata": {},
   "source": [
    "# Step 3 - Train and compare multiple algorithms\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0273b53d",
   "metadata": {},
   "source": [
    "### 1. Load preprocessed data\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050dbbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load prepared data from the previous notebook\n",
    "X_train = pd.read_parquet(\"../data/output/X_train.parquet\")\n",
    "X_test = pd.read_parquet(\"../data/output/X_test.parquet\")\n",
    "y_train = pd.read_parquet(\"../data/output/y_train.parquet\").squeeze()\n",
    "y_test = pd.read_parquet(\"../data/output/y_test.parquet\").squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52d582",
   "metadata": {},
   "source": [
    "### 2. Set up cross-validation\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73cd21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Define 5-fold stratified cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e456836",
   "metadata": {},
   "source": [
    "### 3. Train baseline models (Logistic Regression & Random Forest)\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a4f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Dictionary of models to test\n",
    "# Commented : \n",
    "# Logistic Regression: F1-score moyen = 0.1866 (+/- 0.0131)\n",
    "# Random Forest: F1-score moyen = 0.2780 (+/- 0.0157)\n",
    "# SVM: F1-score moyen = 0.1699 (+/- 0.0059)\n",
    "models = {\n",
    "#    \"Logistic Regression\": LogisticRegression(max_iter=500, class_weight='balanced'),\n",
    "#    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=5, class_weight='balanced'),\n",
    "#    \"SVM\": SVC(probability=True, class_weight='balanced'),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric=\"logloss\", scale_pos_weight=10, random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(class_weight=\"balanced\", verbose=-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0475db",
   "metadata": {},
   "source": [
    "### 4. Try stronger models (Boosting, MLP)\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b8a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "def clean_column_names(df):\n",
    "    df.columns = df.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\n",
    "    return df\n",
    "\n",
    "X_train = clean_column_names(X_train)\n",
    "X_test = clean_column_names(X_test)\n",
    "\n",
    "X_sample = X_train.sample(n=5000, random_state=42)\n",
    "y_sample = y_train.loc[X_sample.index]\n",
    "\n",
    "# LightGBM does not support special characters in column names; clean them\n",
    "X_sample.columns = X_sample.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\n",
    "\n",
    "# Score based on F1-score\n",
    "scorer = make_scorer(f1_score, average=\"binary\", pos_label=1)\n",
    "\n",
    "# Evaluate models with cross-validation\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_sample, y_sample, cv=cv, scoring=scorer)\n",
    "    print(f\"{name}: F1-score moyen = {scores.mean():.4f} (+/- {scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00abb97e",
   "metadata": {},
   "source": [
    "### 5. Handle class imbalance\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b71debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "mlflow.set_tracking_uri(\"file:../mlruns\")\n",
    "mlflow.set_experiment(\"modele_classification\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    with mlflow.start_run(run_name=name):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        score = f1_score(y_test, y_pred)\n",
    "        f1_train = f1_score(y_train, model.predict(X_train))\n",
    "\n",
    "        mlflow.log_param(\"model\", name)\n",
    "        mlflow.log_metric(\"f1_train\", f1_train)\n",
    "        mlflow.log_metric(\"f1_test\", score)\n",
    "\n",
    "        signature = infer_signature(X_test, y_pred)\n",
    "        mlflow.sklearn.log_model(model, name.replace(\" \", \"_\").lower(), signature=signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418bef00",
   "metadata": {},
   "source": [
    "### 6. Track experiments with MLflow\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1374f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import joblib\n",
    "\n",
    "# Parameters\n",
    "best_model = XGBClassifier(n_estimators=100, max_depth=5, use_label_encoder=False, eval_metric=\"logloss\")\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "score = f1_score(y_test, y_pred)\n",
    "\n",
    "# Tracking MLflow\n",
    "with mlflow.start_run(run_name=\"XGBoost_best_model\"):\n",
    "    # Log hyperparameters\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"max_depth\", 5)\n",
    "    mlflow.log_param(\"model_type\", \"XGBoost\")\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"f1_score\", score)\n",
    "\n",
    "    # Input/output signature\n",
    "    signature = infer_signature(X_test, y_pred)\n",
    "\n",
    "    # Log du model\n",
    "    mlflow.sklearn.log_model(best_model, \"xgboost_best_model\", signature=signature)\n",
    "\n",
    "    # Optional: add tags for traceability\n",
    "    mlflow.set_tags({\n",
    "        \"stage\": \"final_model\",\n",
    "        \"author\": \"David Worsley-Tonks\",\n",
    "        \"model\": \"XGBoost\",\n",
    "        \"version\": \"v1\"\n",
    "    })\n",
    "\n",
    "# Sauvegarde locale\n",
    "joblib.dump(best_model, \"../models/credit_scoring_xgb.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d986c49",
   "metadata": {},
   "source": [
    "### 7. Compare performance and select a model\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0447f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Comparaison des performances des models sur le test set\n",
    "scores = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    scores.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1-score\": f1_score(y_test, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_scores = pd.DataFrame(scores).sort_values(by=\"F1-score\", ascending=False)\n",
    "display(df_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da3f166",
   "metadata": {},
   "source": [
    "### 8. Explore results in the MLflow UI\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa60d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "\n",
    "# Open the MLflow UI locally\n",
    "webbrowser.open(\"http://localhost:8889\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a9d400",
   "metadata": {},
   "source": [
    "### 9. Conclusion\n\nDetails and rationale are implemented in the code cells below.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}