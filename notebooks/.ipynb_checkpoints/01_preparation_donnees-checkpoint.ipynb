{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac459a09",
   "metadata": {},
   "source": [
    "# Étape 1 — Préparation et nettoyage des données\n",
    "\n",
    "L'objectif de cette première étape est d'obtenir un **jeu de données propre, fusionné et enrichi**, prêt à être utilisé pour l'entraînement des modèles de machine learning. Pour cela, nous allons :\n",
    "\n",
    "1. Charger et explorer les jeux de données bruts.\n",
    "2. Identifier les colonnes clés pour effectuer les jointures.\n",
    "3. Nettoyer les valeurs manquantes et les doublons.\n",
    "4. Réaliser les jointures pour constituer un dataset final unique.\n",
    "\n",
    "Nous prendrons également en compte les spécificités métiers, comme la présence de classes déséquilibrées ou de colonnes ID spécifiques à certaines tables.\n",
    "\n",
    "> **Note** : Ce notebook s'inspire en partie de l'approche proposée par Will Koehrsen dans le cadre du challenge Home Credit.  \n",
    "> Source : [Start Here - A Gentle Introduction](https://www.kaggle.com/code/willkoehrsen/start-here-a-gentle-introduction/notebook)\n",
    ">\n",
    "> Nous reprenons notamment certaines bonnes pratiques de structuration, de visualisation des valeurs manquantes, et d’agrégation intelligente des jeux de données secondaires.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e3a4c",
   "metadata": {},
   "source": [
    "## 1. Chargement des fichiers de données\n",
    "Les fichiers sont stockés dans le dossier `../data/raw/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf0c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "raw_data_path = Path('../data/raw/')\n",
    "files = list(raw_data_path.glob('*.csv'))\n",
    "datasets = {}\n",
    "\n",
    "# Encodage utf-8 avec gestion des erreurs\n",
    "for file in files:\n",
    "    try:\n",
    "        df = pd.read_csv(file, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(file, encoding='ISO-8859-1')\n",
    "    datasets[file.stem] = df\n",
    "    print(f\"{file.name}: {df.shape[0]} lignes, {df.shape[1]} colonnes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2da86",
   "metadata": {},
   "source": [
    "## 2. Vérification et correction des formats\n",
    "\n",
    "Avant de procéder à tout nettoyage ou fusion, il est essentiel de s'assurer que les jeux de données ont des types cohérents et que les formats sont exploitables.\n",
    "\n",
    "#### Actions menées :\n",
    "- Vérification des types de colonnes (`df.dtypes`)\n",
    "- Conversion explicite des dates (`object` → `datetime`)\n",
    "- Homogénéisation des types numériques (`float32`, `int64`)\n",
    "- Détection de colonnes booléennes encodées sous forme de `0/1` ou de chaînes\n",
    "\n",
    "#### Résultat :\n",
    "Toutes les colonnes critiques (dates, booléens, numériques) ont un format adapté à l’analyse ou à la modélisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3485653",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in datasets.items():\n",
    "    print(f\"\\n==== {name.upper()} : types initiaux ====\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    # Exemple : conversion des colonnes 'DAYS_BIRTH' en valeur absolue\n",
    "    if 'DAYS_BIRTH' in df.columns:\n",
    "        df['DAYS_BIRTH'] = df['DAYS_BIRTH'].abs()\n",
    "    \n",
    "    # Conversion des colonnes contenant 'DATE' ou 'datetime' si besoin\n",
    "    for col in df.select_dtypes('object'):\n",
    "        if 'date' in col.lower() or 'datetime' in col.lower():\n",
    "            try:\n",
    "                df[col] = pd.to_datetime(df[col])\n",
    "                print(f\"{col} convertie en datetime\")\n",
    "            except:\n",
    "                pass  # on ignore si ce n’est pas convertible\n",
    "\n",
    "    # Mise à jour du dictionnaire\n",
    "    datasets[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a9ce96",
   "metadata": {},
   "source": [
    "## 3. Traitement des valeurs manquantes et doublons\n",
    "\n",
    "Cette étape a pour objectif de :\n",
    "\t\t\n",
    "- Identifier visuellement les valeurs manquantes dans chaque fichier\n",
    "- Supprimer les colonnes contenant trop de valeurs manquantes\n",
    "- Imputer les valeurs manquantes restantes selon le type de variable\n",
    "- Supprimer les doublons éventuels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c075bbd",
   "metadata": {},
   "source": [
    "### 3.1 Visualisation des valeurs manquantes\n",
    "\n",
    "Visualiser rapidement la structure des valeurs manquantes par fichier pour anticiper les nettoyages nécessaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22afded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    na_ratio = df.isna().mean().sort_values(ascending=False)\n",
    "    print(\"Variables avec > 50% de valeurs manquantes :\")\n",
    "    display(na_ratio[na_ratio > 0.5])\n",
    "\n",
    "    msno.matrix(df)\n",
    "    plt.title(f\"Missing values — {name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8541cc0",
   "metadata": {},
   "source": [
    "### 3.2 Nettoyage : suppression et imputation\n",
    "\n",
    "Nettoyer les données en supprimant les colonnes peu exploitables et en imputant les valeurs manquantes restantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e62482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n=== Nettoyage du dataset : {name.upper()} ===\")\n",
    "\n",
    "    # 1. Supprimer les colonnes avec plus de 80% de NaN\n",
    "    threshold = 0.8\n",
    "    missing_ratio = df.isna().mean()\n",
    "    cols_to_drop = missing_ratio[missing_ratio > threshold].index\n",
    "    df.drop(columns=cols_to_drop, inplace=True)\n",
    "    print(f\"{len(cols_to_drop)} colonnes supprimées (>80% NaN) : {list(cols_to_drop)}\")\n",
    "\n",
    "    # 2. Imputer les variables numériques (par la médiane)\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    imputer_num = SimpleImputer(strategy='median')\n",
    "    df[num_cols] = imputer_num.fit_transform(df[num_cols])\n",
    "\n",
    "    # 3. Imputer les variables catégorielles (par \"inconnu\")\n",
    "    cat_cols = df.select_dtypes(include='object').columns\n",
    "    df[cat_cols] = df[cat_cols].fillna(\"inconnu\")\n",
    "\n",
    "    # Mise à jour dans le dictionnaire\n",
    "    datasets[name] = df\n",
    "\n",
    "    print(f\"{len(num_cols)} colonnes numériques imputées\")\n",
    "    print(f\"{len(cat_cols)} colonnes catégorielles imputées\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2a8a00",
   "metadata": {},
   "source": [
    "### 3.3 Suppression des doublons\n",
    "\n",
    "Éliminer les éventuelles lignes dupliquées qui pourraient biaiser l’analyse ou l’entraînement du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e962ab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in datasets.items():\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        print(f\"{name}: {duplicates} doublons supprimés\")\n",
    "    else:\n",
    "        print(f\"{name}: aucun doublon détecté\")\n",
    "\n",
    "    # Mise à jour\n",
    "    datasets[name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0214fc90",
   "metadata": {},
   "source": [
    "## 4. Identification des colonnes clés pour les jointures\n",
    "\n",
    "Pour fusionner les différents jeux de données, il est essentiel d’identifier les colonnes servant de clés primaires et secondaires. Ces clés permettent de relier les historiques d’interactions du client (`bureau`, `previous_application`, `credit_card_balance`, etc.) à l’enregistrement principal d’un client unique.\n",
    "\n",
    "Nous nous appuyons ici :\n",
    "- Sur l’analyse de la documentation fournie (`HomeCredit_columns_description.csv`),\n",
    "- Sur la structure des noms de colonnes (`SK_ID_CURR`, `SK_ID_PREV`, `SK_ID_BUREAU`),\n",
    "- Sur la logique métier (par exemple, une carte de crédit appartient à une demande passée, elle-même reliée à un client).\n",
    "\n",
    "**Remarque** : le fichier `HomeCredit_columns_description.csv` est un document de **description des colonnes**. Il ne comporte **aucune clé de jointure** et ne sera donc **pas fusionné** avec les autres jeux de données.\n",
    "\n",
    "Le tableau ci-dessous récapitule les relations entre fichiers, et un dictionnaire Python permet de centraliser ces clés pour automatiser les jointures ultérieures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57768dc0",
   "metadata": {},
   "source": [
    "| Fichier              | Clé principale       | Clé(s) de jointure         |\n",
    "|----------------------|----------------------|----------------------------|\n",
    "| application_train    | SK_ID_CURR           | —                          |\n",
    "| application_test     | SK_ID_CURR           | —                          |\n",
    "| bureau               | SK_ID_BUREAU         | SK_ID_CURR                 |\n",
    "| bureau_balance       | SK_ID_BUREAU         | via bureau                 |\n",
    "| previous_application | SK_ID_PREV           | SK_ID_CURR                 |\n",
    "| POS_CASH_balance     | SK_ID_PREV           | via previous_application   |\n",
    "| installments_payments| SK_ID_PREV           | via previous_application   |\n",
    "| credit_card_balance  | SK_ID_PREV           | via previous_application   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d38fee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colonnes clés identifiées manuellement à partir de la documentation\n",
    "primary_keys = {\n",
    "    'application_train': 'SK_ID_CURR',\n",
    "    'application_test': 'SK_ID_CURR',\n",
    "    'bureau': 'SK_ID_CURR',\n",
    "    'bureau_balance': 'SK_ID_BUREAU',\n",
    "    'previous_application': 'SK_ID_CURR',\n",
    "    'installments_payments': 'SK_ID_PREV',\n",
    "    'credit_card_balance': 'SK_ID_PREV',\n",
    "    'POS_CASH_balance': 'SK_ID_PREV',\n",
    "    'sample_submission': 'SK_ID_CURR'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1544efaa",
   "metadata": {},
   "source": [
    "## 5. Fusionner les jeux de données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90bb719",
   "metadata": {},
   "source": [
    "### 5.1 Fusion des données Bureau \n",
    "\n",
    "Nous commençons par fusionner les données issues des fichiers bureau_balance et bureau, en suivant la logique relationnelle identifiée à l’étape 4 :\n",
    "- Agrégation des historiques bureau_balance au niveau de chaque SK_ID_BUREAU (via moyenne et somme),\n",
    "- Fusion avec bureau sur la colonne SK_ID_BUREAU,\n",
    "- Puis fusion avec application_train sur SK_ID_CURR.\n",
    "\n",
    "Cela permet d’enrichir chaque client avec un résumé de ses interactions passées via les bureaux de crédit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d53ae88-bff4-4b36-b9f6-7a3a9c5b5d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_numeric_only(df, group_var, df_name):\n",
    "    \"\"\"\n",
    "    Agrège uniquement les colonnes numériques d’un DataFrame par une clé donnée.\n",
    "    Les noms des colonnes sont préfixés pour éviter les collisions.\n",
    "    \"\"\"\n",
    "    df_numeric = df.select_dtypes(include=['number']).copy()\n",
    "    df_numeric[group_var] = df[group_var]\n",
    "    agg = df_numeric.groupby(group_var).agg(['mean', 'sum'])\n",
    "    agg.columns = [f\"{df_name}_{col[0]}_{col[1]}\" for col in agg.columns]\n",
    "    agg.reset_index(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5489bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion bureau_balance → bureau\n",
    "bureau_balance = datasets['bureau_balance']\n",
    "bureau = datasets['bureau']\n",
    "\n",
    "bureau_balance_agg = agg_numeric_only(bureau_balance, 'SK_ID_BUREAU', 'BB')\n",
    "bureau = bureau.merge(bureau_balance_agg, on='SK_ID_BUREAU', how='left')\n",
    "\n",
    "# Fusion bureau → application\n",
    "app_train = datasets['application_train']\n",
    "app_train = app_train.merge(bureau.drop(columns='SK_ID_BUREAU'), on='SK_ID_CURR', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13149e",
   "metadata": {},
   "source": [
    "### 5.2 Fusion des données Previous + historiques\n",
    "\n",
    "Même logique que précédemment, cette fois appliquée aux données liées aux demandes précédentes :\n",
    "- Pour chaque table liée à une demande passée (POS_CASH_balance, installments_payments, credit_card_balance), nous :\n",
    "  - Regroupons les données par SK_ID_PREV\n",
    "  - Calculons des agrégats (moyenne, somme)\n",
    "- Ces agrégats sont fusionnés avec previous_application\n",
    "- Puis previous_application est fusionné à application_train via SK_ID_CURR\n",
    "\n",
    "Ainsi, chaque client est enrichi avec un résumé de ses demandes passées et de leurs historiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8eed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous = datasets['previous_application']\n",
    "\n",
    "# Agrégats pour les historiques liés à previous\n",
    "for table_name in ['POS_CASH_balance', 'installments_payments', 'credit_card_balance']:\n",
    "    df = datasets[table_name]\n",
    "    df_agg = agg_numeric_only(df, 'SK_ID_PREV', table_name)\n",
    "    previous = previous.merge(df_agg, on='SK_ID_PREV', how='left')\n",
    "\n",
    "# Fusion avec application\n",
    "app_train = app_train.merge(previous.drop(columns='SK_ID_PREV'), on='SK_ID_CURR', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8298c1",
   "metadata": {},
   "source": [
    "### 5.3 Export final\n",
    "\n",
    "Le fichier enrichi final est exporté dans ./data/output/train_clean_merged.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c9076",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path('../data/output')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "app_train.to_csv(output_path / 'train_clean_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac5da0e",
   "metadata": {},
   "source": [
    "⚠️ Suite à un échange avec mon mentor (Elie Wanko), nous avons décidé de ne pas utiliser l'intégralité du fichier `train_clean_merged.csv` pour les étapes de modélisation.\n",
    "\n",
    "Le fichier final, issu de la fusion des jeux de données bruts, atteint environ 10. Go. Ce volume très important est susceptible de ralentir, voire bloquer notre environnement d'exécution (Jupyter Lab / MLFlow).\n",
    "\n",
    "Afin d’assurer la faisabilité de nos expérimentations tout en conservant un volume représentatif, nous avons choisi de diviser ce fichier en deux et de conserver uniquement la première moitié pour la suite. Ce fichier allégé reste suffisant pour entraîner, valider et comparer plusieurs modèles dans le cadre du projet.\n",
    "\n",
    "Nous enregistrons donc un fichier `train_clean_sample.csv`, contenant 12.5 % des lignes originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b98069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compter le nombre total de lignes (moins l'en-tête)\n",
    "with open(\"../data/output/train_clean_merged.csv\", \"r\") as f:\n",
    "    total_lines = sum(1 for line in f) - 1\n",
    "\n",
    "# Calculer combien garder (1/8 du fichier)\n",
    "eighth_lines = total_lines // 8\n",
    "\n",
    "# Lire et sauvegarder 1/8 des données\n",
    "df_sample = pd.read_csv(\"../data/output/train_clean_merged.csv\", nrows=eighth_lines)\n",
    "df_sample.to_csv(\"../data/output/train_clean_sample.csv\", index=False)\n",
    "\n",
    "# Supprimer le fichier trop volumineux\n",
    "import os\n",
    "os.remove(\"../data/output/train_clean_merged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92a0110",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Dans cette première étape, nous avons réalisé une préparation complète des données en suivant les bonnes pratiques de la data science appliquée à la modélisation supervisée :\n",
    "\n",
    "- Exploration initiale des jeux de données fournis\n",
    "- Vérification des formats, des valeurs manquantes et des doublons\n",
    "- Identification des colonnes clés pour les jointures (`SK_ID_CURR`, `SK_ID_BUREAU`, `SK_ID_PREV`)\n",
    "- Nettoyage des colonnes fortement manquantes\n",
    "- Imputation de valeurs (moyenne, médiane ou catégories “unknown”)\n",
    "- Fusion cohérente de toutes les sources secondaires avec application_train\n",
    "\n",
    "Le jeu final `train_clean_merged.csv` est un dataset propre, enrichi et prêt à être utilisé pour l’entraînement d’un modèle de scoring."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
