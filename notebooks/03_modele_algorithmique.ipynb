{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e209691a",
   "metadata": {},
   "source": [
    "# Étape 3 – Modélisez et expérimentez avec plusieurs algorithmes\n",
    "\n",
    "Dans cette étape, nous allons entraîner plusieurs modèles de classification, comparer leurs performances sur des métriques classiques et métier, et tracer toutes nos expérimentations avec MLflow. L’objectif est de construire une première version robuste du pipeline d’apprentissage automatique, intégrant la validation croisée, la gestion du déséquilibre des classes et la traçabilité des modèles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0273b53d",
   "metadata": {},
   "source": [
    "### 1. Chargement des données pré-traitées\n",
    "\n",
    "Nous chargeons ici les jeux de données `X_train`, `X_test`, `y_train`, `y_test` préparés à l’étape précédente. Il est important de valider que les formats sont corrects et de vérifier la distribution des classes, notamment si le dataset est déséquilibré."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050dbbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger les données préparées depuis le notebook précédent\n",
    "X_train = pd.read_parquet(\"../data/output/X_train.parquet\")\n",
    "X_test = pd.read_parquet(\"../data/output/X_test.parquet\")\n",
    "y_train = pd.read_parquet(\"../data/output/y_train.parquet\").squeeze()\n",
    "y_test = pd.read_parquet(\"../data/output/y_test.parquet\").squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52d582",
   "metadata": {},
   "source": [
    "### 2. Mise en place d’une stratégie de validation croisée\n",
    "\n",
    "Nous définissons une stratégie de validation robuste avec `StratifiedKFold`, afin de garantir une répartition homogène des classes cibles dans les splits d’entraînement et de validation. Cela permet de mieux évaluer la généralisation des modèles, en particulier dans le cas de classes déséquilibrées.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73cd21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Définition d'une validation croisée stratifiée à 5 plis\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e456836",
   "metadata": {},
   "source": [
    "### 3. Entraînement de modèles de base (Logistic Regression & Random Forest)\n",
    "\n",
    "Nous commençons par entraîner des modèles simples mais efficaces : \n",
    "- **Régression logistique**, avec pénalisation des classes minoritaires.\n",
    "- **Forêt aléatoire**, avec des hyperparamètres de base.\n",
    "\n",
    "Chaque modèle sera évalué à l’aide de la validation croisée et ses performances seront tracées dans MLflow (paramètres, scores, modèle).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a4f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# Dictionnaire des modèles à tester\n",
    "# Commented : \n",
    "# Logistic Regression: F1-score moyen = 0.1866 (+/- 0.0131)\n",
    "# Random Forest: F1-score moyen = 0.2780 (+/- 0.0157)\n",
    "# SVM: F1-score moyen = 0.1699 (+/- 0.0059)\n",
    "models = {\n",
    "#    \"Logistic Regression\": LogisticRegression(max_iter=500, class_weight='balanced'),\n",
    "#    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=5, class_weight='balanced'),\n",
    "#    \"SVM\": SVC(probability=True, class_weight='balanced'),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric=\"logloss\", scale_pos_weight=10, random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(class_weight=\"balanced\", verbose=-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0475db",
   "metadata": {},
   "source": [
    "### 4. Test de modèles plus puissants (Boosting, MLP…)\n",
    "\n",
    "Nous expérimentons ici des modèles plus avancés comme :\n",
    "- **XGBoostClassifier**\n",
    "- **LightGBMClassifier**\n",
    "- (optionnel) **MLPClassifier**\n",
    "\n",
    "Ces modèles sont généralement plus performants mais demandent une gestion plus fine des hyperparamètres et du surapprentissage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b8a617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "def clean_column_names(df):\n",
    "    df.columns = df.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\n",
    "    return df\n",
    "\n",
    "X_train = clean_column_names(X_train)\n",
    "X_test = clean_column_names(X_test)\n",
    "\n",
    "X_sample = X_train.sample(n=5000, random_state=42)\n",
    "y_sample = y_train.loc[X_sample.index]\n",
    "\n",
    "# LightGBM ne supporte pas les colonnes avec caractères spéciaux → on les nettoie\n",
    "X_sample.columns = X_sample.columns.str.replace('[^A-Za-z0-9_]+', '_', regex=True)\n",
    "\n",
    "# Score basé sur le F1-score\n",
    "scorer = make_scorer(f1_score, average=\"binary\", pos_label=1)\n",
    "\n",
    "# Évaluer les modèles avec validation croisée\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, X_sample, y_sample, cv=cv, scoring=scorer)\n",
    "    print(f\"{name}: F1-score moyen = {scores.mean():.4f} (+/- {scores.std():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00abb97e",
   "metadata": {},
   "source": [
    "### 5. Gestion du déséquilibre des classes\n",
    "\n",
    "Nous intégrons ici deux approches complémentaires :\n",
    "- La pondération des classes via `class_weight=\"balanced\"`\n",
    "- Le sur-échantillonnage de la classe minoritaire avec **SMOTE**\n",
    "\n",
    "Nous comparons les résultats obtenus avec et sans traitement du déséquilibre, et analysons leur impact sur les scores métiers (recall, f1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b71debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "mlflow.set_tracking_uri(\"file:../mlruns\")\n",
    "mlflow.set_experiment(\"modele_classification\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    with mlflow.start_run(run_name=name):\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        score = f1_score(y_test, y_pred)\n",
    "        f1_train = f1_score(y_train, model.predict(X_train))\n",
    "\n",
    "        mlflow.log_param(\"model\", name)\n",
    "        mlflow.log_metric(\"f1_train\", f1_train)\n",
    "        mlflow.log_metric(\"f1_test\", score)\n",
    "\n",
    "        signature = infer_signature(X_test, y_pred)\n",
    "        mlflow.sklearn.log_model(model, name.replace(\" \", \"_\").lower(), signature=signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418bef00",
   "metadata": {},
   "source": [
    "### 6. Enregistrement des expérimentations avec MLflow\n",
    "\n",
    "Chaque run est tracé avec MLflow :\n",
    "- Paramètres (`log_param`)\n",
    "- Métriques (`log_metric`)\n",
    "- Modèle (`log_model`)\n",
    "- Métadonnées utiles (`set_tags`)\n",
    "\n",
    "Nous rendons ainsi chaque expérimentation traçable, comparable, et potentiellement réutilisable dans une chaîne d’industrialisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1374f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from mlflow.models.signature import infer_signature\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import joblib\n",
    "\n",
    "# Paramètres\n",
    "best_model = XGBClassifier(n_estimators=100, max_depth=5, use_label_encoder=False, eval_metric=\"logloss\")\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions\n",
    "y_pred = best_model.predict(X_test)\n",
    "score = f1_score(y_test, y_pred)\n",
    "\n",
    "# Tracking MLflow\n",
    "with mlflow.start_run(run_name=\"XGBoost_best_model\"):\n",
    "    # Log des hyperparamètres\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"max_depth\", 5)\n",
    "    mlflow.log_param(\"model_type\", \"XGBoost\")\n",
    "\n",
    "    # Log des métriques\n",
    "    mlflow.log_metric(\"f1_score\", score)\n",
    "\n",
    "    # Signature d'entrée/sortie\n",
    "    signature = infer_signature(X_test, y_pred)\n",
    "\n",
    "    # Log du modèle\n",
    "    mlflow.sklearn.log_model(best_model, \"xgboost_best_model\", signature=signature)\n",
    "\n",
    "    # Optionnel : ajout de tags pour traçabilité\n",
    "    mlflow.set_tags({\n",
    "        \"stage\": \"final_model\",\n",
    "        \"author\": \"David Worsley-Tonks\",\n",
    "        \"model\": \"XGBoost\",\n",
    "        \"version\": \"v1\"\n",
    "    })\n",
    "\n",
    "# Sauvegarde locale\n",
    "joblib.dump(best_model, \"../models/best_model_xgb.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d986c49",
   "metadata": {},
   "source": [
    "### 7. Comparaison des performances et sélection de modèle\n",
    "\n",
    "Nous rassemblons ici les scores clés (accuracy, recall, f1, AUC) de tous les modèles testés. Une comparaison objective est présentée sous forme de tableau. L’objectif est de sélectionner le ou les modèles les plus adaptés pour la suite du projet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0447f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Comparaison des performances des modèles sur le test set\n",
    "scores = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    scores.append({\n",
    "        \"Modèle\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1-score\": f1_score(y_test, y_pred),\n",
    "        \"AUC\": roc_auc_score(y_test, y_proba) if y_proba is not None else None\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_scores = pd.DataFrame(scores).sort_values(by=\"F1-score\", ascending=False)\n",
    "display(df_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da3f166",
   "metadata": {},
   "source": [
    "### 8. Exploration des résultats dans l’interface MLflow\n",
    "\n",
    "L’interface MLflow (accessible via `http://localhost:8889`) permet de :\n",
    "- Visualiser l’historique des runs\n",
    "- Comparer les performances\n",
    "- Retrouver les paramètres et modèles associés\n",
    "- Faciliter le versioning des expérimentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa60d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "\n",
    "# Ouvrir automatiquement l’interface MLflow (en local)\n",
    "webbrowser.open(\"http://localhost:8889\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a9d400",
   "metadata": {},
   "source": [
    "### 9. Conclusion\n",
    "\n",
    "Au cours de cette étape, nous avons comparé plusieurs familles de modèles de classification supervisée, notamment Random Forest, XGBoost et LightGBM. Nous avons mis en place une stratégie rigoureuse de validation croisée pour comparer les modèles sur des métriques adaptées à notre problème déséquilibré (notamment le F1-score).\n",
    "\n",
    "Nous avons également intégré MLflow afin de tracer chaque expérimentation de manière transparente : enregistrement des paramètres, scores et modèles dans une interface dédiée. Cette démarche permet de faciliter l’analyse comparative des performances, de suivre l’évolution des expérimentations, et de poser une base solide pour les étapes suivantes.\n",
    "\n",
    "Les résultats montrent que XGBoost obtient les meilleures performances globales, avec un F1-score, une précision et un rappel élevés sur notre jeu de test.\n",
    "\n",
    "Grâce à cette étape, nous avons construit un socle robuste pour la suite du projet : amélioration des modèles, tuning d’hyperparamètres, interprétabilité, et industrialisation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
